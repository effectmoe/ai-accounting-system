#!/usr/bin/env tsx

/**
 * AI Cascade Test Script
 * AIã‚«ã‚¹ã‚±ãƒ¼ãƒ‰æ©Ÿèƒ½ã®å‹•ä½œç¢ºèª
 */

import 'dotenv/config';
import { LLMCascadeManager } from '../lib/llm-cascade-manager';

async function testAICascade() {
  console.log('ðŸ§ª Testing AI Cascade System...\n');

  const aiManager = new LLMCascadeManager();

  // ç°¡å˜ãªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ†ã‚¹ãƒˆ
  console.log('ðŸ’¬ Testing text generation...');
  
  try {
    const response = await aiManager.generateText(
      'ã“ã‚“ã«ã¡ã¯ã€‚ç°¡å˜ã«è‡ªå·±ç´¹ä»‹ã—ã¦ãã ã•ã„ã€‚',
      'ã‚ãªãŸã¯è¦ªåˆ‡ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ç°¡æ½”ã«ç­”ãˆã¦ãã ã•ã„ã€‚'
    );

    console.log('âœ… Response received:');
    console.log(response);
    console.log('\n');

    // LLM Requestå½¢å¼ã§ã®ãƒ†ã‚¹ãƒˆ
    console.log('ðŸ“ Testing with LLM Request format...');
    const llmResponse = await aiManager.generateResponse({
      messages: [
        {
          role: 'system',
          content: 'ã‚ãªãŸã¯ä¼šè¨ˆã®å°‚é–€å®¶ã§ã™ã€‚',
        },
        {
          role: 'user',
          content: 'å£²ä¸Šé«˜ã¨å£²ä¸ŠåŽŸä¾¡ã®é•ã„ã‚’ç°¡å˜ã«èª¬æ˜Žã—ã¦ãã ã•ã„ã€‚',
        },
      ],
      temperature: 0.7,
      maxTokens: 200,
    });

    console.log('âœ… LLM Response:');
    console.log('Provider:', llmResponse.provider);
    console.log('Success:', llmResponse.success);
    console.log('Content:', llmResponse.content);
    console.log('Usage:', llmResponse.usage);
    
  } catch (error) {
    console.error('âŒ Error during AI response generation:', error);
  }

  console.log('\nâœ… AI Cascade test completed!');
}

// ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
testAICascade().catch((error) => {
  console.error('Test failed:', error);
  process.exit(1);
});